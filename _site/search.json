[
  {
    "objectID": "04_preprocessing.html",
    "href": "04_preprocessing.html",
    "title": "Preprocessing Steps",
    "section": "",
    "text": "You‚Äôve probably heard the phrase ‚Äúgarbage in, garbage out‚Äù, right? It‚Äôs a core principle in computing: the quality of the output heavily depends on the quality of the input.\nThis concept holds especially true in sentiment analysis and other natural language processing (NLP) tasks because human language is naturally messy, inconsistent, and often ambiguous.\nTo perform accurate and reliable analysis, we need to ‚Äútake out the garbage‚Äù first by preprocessing the text to clean, standardize, and structure the input data. This reduces noise and improves the model‚Äôs accuracy. Key text preprocessing steps include normalization, stop words removal, tokenization and lemmatization, which are depicted and explained in the handout below:\n\n\nSource: Data Literacy Series https://perma.cc/L8U5-ZEXD\nIn the next episodes, we‚Äôll dive deeper into this NLP pipeline to prepare the data for sentiment analysis by exploring each of these steps with more detailed explanations.",
    "crumbs": [
      "Text Preprocessing"
    ]
  },
  {
    "objectID": "03_inspecting.html",
    "href": "03_inspecting.html",
    "title": "Inspecting the Data",
    "section": "",
    "text": "After all, producers may also be watching the public‚Äôs reaction on social media to decide whether a second season is worth pursuing.\nThe data we pulled for this exercise comes from real social media posts, meaning they are inherently messy, and we know that even before going in. Because it is derived from natural language, this kind of data is unstructured, often filled with inconsistencies and irregularities.\nBefore we can apply any meaningful analysis or modeling, it‚Äôs crucial to visually inspect the data to get a sense of what we‚Äôre working with. Eyeballing the raw text helps us identify common patterns, potential noise, and areas that will require careful preprocessing to ensure the downstream tasks are effective and reliable.\nIt is time to open RStudio and our example! The folder you have saved contains a quarto document (.qmd) which will serve as our workshop workbook. In this document we will include the R code and comments related to the content we will explore. Let‚Äôs have that open in RStudio.\nLet‚Äôs install the required packages (via the console) and load them. Next, let‚Äôs inspect the comments.csv file and take a quick look at it!\n# Inspecting the data\n\ncomments &lt;- read_csv(\"comments.csv\")\nhead(comments$text)\n\n\n\n\n\n\nüí¨ Discussion\n\n\n\nWorking in pairs or trios, look briefly at the data and discuss the challenges that may arise when attempting to analyze this dataset on its current form. What could be potential areas of friction that could compromise the results?",
    "crumbs": [
      "Inspecting the Data"
    ]
  },
  {
    "objectID": "00_about.html",
    "href": "00_about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "00_about.html#ways-we-can-help-you",
    "href": "00_about.html#ways-we-can-help-you",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "about.html#ways-we-can-help-you",
    "href": "about.html#ways-we-can-help-you",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "06_token.html",
    "href": "06_token.html",
    "title": "Tokenization",
    "section": "",
    "text": "Image from: Muhammad Arslan Shahzad (2025)\nTokenization in NLP differs from applications in security and blockchain. It corresponds to the action of breaking down text into smaller pieces (aka tokens). It is a foundational process in the digital world, allowing machines to interpret and analyze large volumes of text data. By dividing text into smaller, more manageable units, it enhances both the efficiency and accuracy of data processing.\nText can be tokenized into sentences, word, subwords or even characters, depending on project goals and analysis plan. Here is a summary of these approaches:\nSome might recall, that along with the popularization and excitement around ChaGPT, there were also a few warnings about the LLMs failing in answering correctly how many ‚Äúr‚Äù letters does the word strawberry have. Can you guess why?\nAlthough this issue has been resolved in later versions of the model, it was originally caused by subword tokenization. In this case, the tokenizer would split ‚Äústrawberry‚Äù into ‚Äúst,‚Äù ‚Äúraw,‚Äù and ‚Äúberry.‚Äù As a result, the model would incorrectly count the letter ‚Äúr‚Äù only within the ‚Äúberry‚Äù token. This illustrates how the tokenization approach directly affects how words are segmented and how their components are interpreted by the model.\nWhile this is beyond the scope of the workshop, it‚Äôs important to note that some advanced AI models use neural networks to dynamically determine token segmentation. Rather than relying on fixed rules, these models can adapt based on the contextual cues within the text. However, tokenization remains inherently limited by the irregular, organic, and often unpredictable nature of human language.",
    "crumbs": [
      "Text Preprocessing",
      "Tokenizing Words"
    ]
  },
  {
    "objectID": "06_token.html#part-of-speech-pos-tagging-word-embeddings",
    "href": "06_token.html#part-of-speech-pos-tagging-word-embeddings",
    "title": "Tokenization",
    "section": "Part of Speech (POS) Tagging & Word Embeddings",
    "text": "Part of Speech (POS) Tagging & Word Embeddings\nIn English language there are eight categories that help structure the meaning of the sentences: nouns, pronouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections.\nIn NLP, POS tagging is the process of labeling each word in a sentence with its corresponding one of these categories.This is a fundamental step in many NLP tasks because it adds syntactic structure to raw text, allowing machines to better understand language. It not only helps computers to correctly interpret sentence structure, but also is key to disambiguate words that might have multiple meanings. The word ‚Äúbook‚Äù for example, can be a noun (‚ÄúI read this book a long time ago‚Äù) or a verb (‚ÄúI still need to book my flight ticket‚Äù) depending on the context.\nBut how the POS tagging process can be reliable? How can the computer know not only the grammatical role of each word in a sentence, but most importantly its semantic meaning in that given contexts?\nWhile we humans don‚Äôt have much trouble making that distinction, computers still struggle to get it right without some help. Enters: word embeddings. Word Embeddings are numerical representations of words (or other textual units like sentences or documents) in a continuous vector space. These representations capture the meaning and relationships between words: words that are closer in the vector space are generally considered to be more similar in meaning.¬†It lets computers understand similarities and learn meanings from usage, not definitions, even when they belong to the same part of speech. For example:\n‚ÄúThat was such a sweet gesture from her‚Äù\n‚ÄúThe cake was too sweet for my taste‚Äù\nBoth are adjectives, but in context with other words, POS tagging and embeddings lets models distinguish emotional sweet from flavor sweet. This because, embeddings connects emotional ‚Äúsweet‚Äù with ‚Äúkind‚Äù or ‚Äúthoughtful,‚Äù not just sugar-related words.\nLet‚Äôs complete the quick quiz below and then move back to our worksheet to remove stop words.\n\n\n\n\n\n\nüß† Knowledge check\n\n\n\nWhich statement is true about its corresponding NLP technique?\na) Tokenization: Breaks text into smaller units like words or punctuation so they can be processed individually.\nb) Part-of-Speech (POS) Tagging: Assigns grammatical labels (e.g., noun, verb, adjective) to each token in a sentence.\nc) Word Embeddings: Represent words as dense numerical vectors that capture semantic relationships.\nd) All of the above\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCorrect: d",
    "crumbs": [
      "Text Preprocessing",
      "Tokenizing Words"
    ]
  },
  {
    "objectID": "08_lemma.html",
    "href": "08_lemma.html",
    "title": "Lemmatization",
    "section": "",
    "text": "Source: Created by the author in Canvas\n\n\nAlso known as word reduction, lemmatization is the process of transforming words into their base or dictionary form (lemma) to identify similarities in meaning and usage across different contexts. Take the word ‚Äúrun‚Äù as an example. It can appear in various forms like ‚Äúran‚Äù, ‚Äúruns‚Äù, ‚Äúrunning‚Äù, and ‚Äúrunner‚Äù. But the variations don‚Äôt stop there, as it includes complex forms like ‚Äúoutrun‚Äù, ‚Äúoverrun‚Äù, or ‚Äúunderrun‚Äù. These variations make it challenging for computers to process natural language effectively unless they can recognize that these words are related. That‚Äôs where lemmatization comes in; it helps machines group related words together by reducing them to a common root present in the dictionary, enabling better understanding and analysis of text.\nYou might be asking yourself: Wait‚Ä¶but what about ‚Äústemming‚Äù? Stemming is considered a more straightforward but agressive process that removes prefixes and suffices, reducing the word to its roots. It is considered a less effective approach compared to lemmatization because it may can occasionally lead to meaningless words. For example, while ‚Äúran‚Äù and ‚Äúruns‚Äù would be stemmed to ‚Äúrun‚Äù, for running would end up as ‚Äúrunn‚Äù. For this reason, we will stick with lemmatization and skip stemming in our data analysis. However, depending on your computing capability to process large volumes of textual data, you might consider using it, giving that stemming, since it is considered a more efficient approach.\nAn important thing to consider is that we look into words as separate units (tokens) as we saw in the previous episode. For example, think about the word ‚Äúleaves‚Äù. That could both represent the plural of the noun ‚Äúleaf‚Äù or the verb in third person for the word ‚Äúleave‚Äù. That is a good reminder of always remember to apply part of speech (POS) because lemmatization algorithms utilize a lexicon with linguistic rules based on pre-determined tags to avoid misinterpretation.\n\n\n\n\n\n\nüß† Knowledge Check\n\n\n\nIn pairs or groups of three, apply lemmatization to the following sentence. Identify the base forms (lemmas) of each word:\nCats chasing mice running quickly across gardens.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHow many words did you successfully lemmatize? Bingo if you have identified all key lemmas!\nAfter applying lemmatization, the sentence should look like:\ncat chase mouse run quickly across garden\nNote: Adverbs and prepositions usually remain unchanged because they are already in their simplest dictionary form and do not have a more basic lemma.\n\n\n\n\n\nAlright, back to our pipeline, we will now convert words to their dictionary form, do some\n# Lemmatize\ntokens_nostop &lt;- tokens_nostop %&gt;%\n  mutate(word_lemmatized = lemmatize_words(word))\n\n# Remove single-character words (NOT SURE IF NEEDED)\ntokens_nostop &lt;- tokens_nostop %&gt;%\n  filter(str_length(word_lemmatized) &gt; 1)\n\n# Reconstruct cleaned text per ID \n# (ensure all IDs are preserved, even if empty)\npreprocessed &lt;- data %&gt;%\n  select(id) %&gt;%\n  left_join(\n    tokens_nostop %&gt;%\n      group_by(id) %&gt;%\n      summarise(text_ready = paste(word_lemmatized, collapse = \" \"), .groups = \"drop\"),\n    by = \"id\"\n  ) %&gt;%\n  mutate(text_ready = replace_na(text_ready, \"\"))\n\n# Save to CSV\nwrite_csv(preprocessed, \"preprocessed.csv\")",
    "crumbs": [
      "Text Preprocessing",
      "Lemmatization"
    ]
  },
  {
    "objectID": "09_tf-idf.html",
    "href": "09_tf-idf.html",
    "title": "Understanding Term & Document Frequency",
    "section": "",
    "text": "When performing NLP tasks, including sentiment analysis, we often use a text modeling technique called Bag of Words (BoW) to extract features from documents. In this approach, a document is represented as a ‚Äúbag‚Äù of its words, focusing only on how many times each word appears. The order of the words and their grammatical structure are ignored; what matters is the frequency of each term in the document, as depicted in the figure below:\n\n\n\nPrakash, V. (2023, June 26). An introduction to Bag of Words (BoW). Medium. https://medium.com/@vamshiprakash001/an-introduction-to-bag-of-words-bow-c32a65293ccc\n\n\nThe Bag of Words model is a simple and useful way to turn text into numbers, but it has a limitation since it treats all words with the same frequency as equally important, even if some are far more informative than others. For example, in a collection of product reviews, words like ‚Äúgood‚Äù or ‚Äúproduct‚Äù might appear frequently in almost every review, but they don‚Äôt help us distinguish one review from another.\nThis is where TF-IDF (Term Frequency‚ÄìInverse Document Frequency) comes in. It builds on Bag of Words by keeping track of term frequencies, but also scaling them down for words that are common across the whole dataset, while boosting words that are more unique and meaningful. In short, it captures word importance while also handling common words.\nThink of it like chatting with friends on campus. You might hear ‚ÄúHey,‚Äù ‚ÄúWhat‚Äôs up,‚Äù and ‚ÄúSee you later‚Äù dozens of times a day. These phrases are so common they don‚Äôt tell you who is talking or what the conversation is really about. But if someone says, ‚ÄúThe capstone presentation has been postponed!‚Äù; that‚Äôs rare, specific, and meaningful.\nIn documents, very frequent words act like this everyday chatter which are part of the background noise, but not what makes the exchange noteworthy. If we gave them too much weight, we‚Äôd miss the truly important content.\nThus, the TF-IDF technique aids to noise reduction along with stop word removal, but by using continuous weighting instead of simply deleting words.\n\nTF measures how often a word appears in a given document (in this case, an individual X post).\nIDF measures how rare that word is across all documents (in this case, X posts).\n\nBy multiplying them, TF-IDF highlights words that are both frequent in a specific document and uncommon elsewhere, helping us spot unique patterns and cluster similar posts. For short messages such as X posts and tweets, the IDF component often does most of the work.\nIn short, TF-IDF filters out the constant buzz so the unique and important messages stand out. It tells us how important a word is in a document relative to the entire collection (the corpus).\nBack to our campus example: if you collected students‚Äô feedback on summer programs, words like students, campus, event, and university might dominate your dataset. TF-IDF would lower their weight and boost rarer, more distinctive terms like hackathon, multicultural, or bootcamp.\nIf you want to dive deeper into the math that happens under the hood, this video explains the equations e algorithms behind TF-IDF.\n\nNow, let‚Äôs get back to our worksheet and see how implementing this final preprocessing technique will play out with our dataset.\nInterpreting the table output: (review and use examples from the dataset)\n\n\n\n\n\n\n\nColumn\nMeaning\n\n\n\n\ndocument\nThe identifier of the document. In your case, it‚Äôs 1 for all rows, meaning all words are from the same ‚Äúdocument‚Äù (probably your entire season 1 dataset).\n\n\nword\nThe token (word) in the document.\n\n\nn\nThe raw count of how many times that word appears in the document. For example, severance appears 1802 times.\n\n\ntf\nTerm frequency: the proportion of the word in the document. Calculated as n / total words in the document. For instance, 0.0876 for severance means ~8.76% of all words are ‚Äúseverance‚Äù.\n\n\nidf\nInverse document frequency: measures how unique the word is across all documents. Formula: log(total_docs / docs_containing_word). Here it‚Äôs 0 because you only have one document, so all words appear in every document, making IDF zero.\n\n\ntf_idf\nTF-IDF score: tf * idf. It‚Äôs meant to highlight words that are frequent in a document but rare across documents. Here it‚Äôs 0 for all words because IDF is 0.\n\n\n\nWhile TF-IDF plays a key role in text analysis, it is often less effective for sentiment analysis at the sentence level because it primarily captures term importance rather than the nuanced emotional or contextual meaning of words. TF-IDF is more suitable for tasks such as document classification, keyword extraction, or generating word clouds. For this reason, we will not be using TF-IDF in this workshop.‚Äù",
    "crumbs": [
      "Text Preprocessing",
      "Understanding Term & Document Frequency"
    ]
  },
  {
    "objectID": "01_index.html",
    "href": "01_index.html",
    "title": "Introduction",
    "section": "",
    "text": "Image from Canva",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_index.html#what-is-sentiment-analysis",
    "href": "01_index.html#what-is-sentiment-analysis",
    "title": "Introduction",
    "section": "What is Sentiment Analysis?",
    "text": "What is Sentiment Analysis?\nAs social beings, our beliefs, understanding of reality, and everyday decisions are deeply shaped by the opinions, perceptions and evaluations of others. This social conditioning is a well-documented phenomenon in fields such as psychology, sociology, and communication, where it is understood that individuals often rely on external cues, especially the attitudes and judgments of others when forming their own assessments.\nUnderstanding how public reaction and sentiment shapes and reflects collective perception has become central not only to corporate strategy, but also to scientific inquiry across many academic disciplines.\nWhile the analysis of public opinion predates the Internet, the modern field of sentiment analysis did not gain momentum until the mid-2000s. This surge was largely driven by the rise of Web 2.0, which leveraged the internet into a more participatory platform, enabling users to create, share, and comment on content across chats, blogs, forums, and other social media. These digital spaces dramatically expanded the circulation and accessibility of user-generated content, creating a fertile ground for computational approaches to analyze subjective expressions in large volumes of text. But what is sentiment analysis?\n\nSentiment analysis, also known as opinion mining, is now a well-established area of study within natural language processing (NLP) and computational linguistics. It focuses on identifying and extracting people‚Äôs opinions, evaluations, attitudes, and emotions from written language.\n\nWhether through product reviews, political commentary, or social media posts in virtually any possible topic of interest, sentiment analysis aims to quantify and interpret subjective information at scale, enabling applications in marketing, social science, finance, and beyond. In this course, we will explore ways of extracting insights from textual data, in particular how we can detect underlying emotions within messages shared by people on a popular streaming TV series.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_index.html#learning-goals",
    "href": "01_index.html#learning-goals",
    "title": "Introduction",
    "section": "Learning Goals",
    "text": "Learning Goals\nBy the end of this workshop, participants will be able to:\n\nExplain the fundamental principles and real-world applications of sentiment analysis.\nApply essential text pre-processing techniques to reduce noise and prepare textual data for sentiment analysis.\nPerform sentiment analysis in R using lexicon-based approaches (polarity and emotion detection).\n\nReady to dive in?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02_setup.html",
    "href": "02_setup.html",
    "title": "Setup & Dataset",
    "section": "",
    "text": "For this workshop we will be using R (4.4.1 or above) and RStudio (2024.04.2+764 or above).\n\n\n\nThis workshop requires several packages to assist us with the tasks categorized below:\n\nemoji: Provides functions for converting text to emoji and vice versa, useful in text mining and social media analysis.\nggplot2: A powerful data visualization package based on the Grammar of Graphics, part of the tidyverse.\nlexicon: Offers sentiment lexicons, stopword lists, and other text data dictionaries for NLP and text mining tasks.\nsentimentr: Calculates sentiment scores by considering sentence structure, valence shifters, and negations.\nstopwords: Provides multilingual stopword lists for text preprocessing in NLP pipelines.\nstringi: A comprehensive string processing package supporting Unicode and locale-aware operations.\nsyuzhet: Extracts sentiment and emotion from text using lexicon-based methods, including NRC emotion lexicon for more fine-grained emotion analysis.\ntextclean: Cleans and normalizes text data (e.g., expanding contractions, handling misspellings, and replacing characters).\ntextstem: Performs text stemming and lemmatization for natural language processing.\ntidyr: Provides tools for tidying data, including reshaping, pivoting, and separating columns.\ntidytext: Enables text mining using tidy data principles, including tokenization and binding with tidyverse workflows.\ntidyverse: A collection of R packages (including dplyr, tidyr, ggplot2, readr, etc.) designed for data science workflows.\n\n\n\n\n\n\n\nüì¶ Installing & Loading Packages\n\n\n\nThe code to install and load the required packages is included in the sentiment-analysis.qmd worksheet. However, we will be installing them via the console for one-time setup.",
    "crumbs": [
      "Setup & Dataset"
    ]
  },
  {
    "objectID": "02_setup.html#requirements",
    "href": "02_setup.html#requirements",
    "title": "Setup & Dataset",
    "section": "",
    "text": "For this workshop we will be using R (4.4.1 or above) and RStudio (2024.04.2+764 or above).\n\n\n\nThis workshop requires several packages to assist us with the tasks categorized below:\n\nemoji: Provides functions for converting text to emoji and vice versa, useful in text mining and social media analysis.\nggplot2: A powerful data visualization package based on the Grammar of Graphics, part of the tidyverse.\nlexicon: Offers sentiment lexicons, stopword lists, and other text data dictionaries for NLP and text mining tasks.\nsentimentr: Calculates sentiment scores by considering sentence structure, valence shifters, and negations.\nstopwords: Provides multilingual stopword lists for text preprocessing in NLP pipelines.\nstringi: A comprehensive string processing package supporting Unicode and locale-aware operations.\nsyuzhet: Extracts sentiment and emotion from text using lexicon-based methods, including NRC emotion lexicon for more fine-grained emotion analysis.\ntextclean: Cleans and normalizes text data (e.g., expanding contractions, handling misspellings, and replacing characters).\ntextstem: Performs text stemming and lemmatization for natural language processing.\ntidyr: Provides tools for tidying data, including reshaping, pivoting, and separating columns.\ntidytext: Enables text mining using tidy data principles, including tokenization and binding with tidyverse workflows.\ntidyverse: A collection of R packages (including dplyr, tidyr, ggplot2, readr, etc.) designed for data science workflows.\n\n\n\n\n\n\n\nüì¶ Installing & Loading Packages\n\n\n\nThe code to install and load the required packages is included in the sentiment-analysis.qmd worksheet. However, we will be installing them via the console for one-time setup.",
    "crumbs": [
      "Setup & Dataset"
    ]
  },
  {
    "objectID": "02_setup.html#project-file-sample-data",
    "href": "02_setup.html#project-file-sample-data",
    "title": "Setup & Dataset",
    "section": "Project File & Sample Data",
    "text": "Project File & Sample Data\nOur running example draws on publicly available data from profiles on X (formerly Twitter). Specifically, we collected all posts from the first and second days following the season finales of both Season 1 and Season 2 of the Apple TV series [Severance](https://en.wikipedia.org/wiki/Severance_(TV_series).\nThe data was obtained using Brandwatch, a social media analytics platform subscribed by the UCSB Library (learn more).\nOur search query was carefully constructed to capture relevant posts about the television series Severance, while filtering out unrelated content. We used a combination of keywords to include relevant mentions and exclusion strategies to eliminate noise.\nOur inclusion criteria included ‚Äúseverance‚Äù (in any capitalization), and at least one term related to the show, such as: ‚Äúseries‚Äù, ‚Äúshow‚Äù, ‚ÄúAppleTV‚Äù, ‚ÄúAppleTV+‚Äù, ‚ÄúApple‚Äù, ‚Äúseason‚Äù. To avoid unrelated results‚Äîparticularly those referring to employment severance‚Äîwe excluded posts that mentioned: ‚Äúpackage‚Äù, ‚Äúbenefits‚Äù, ‚Äúlayoff‚Äù, ‚ÄúCigna‚Äù and, ‚Äúexecutive‚Äù; terms are commonly associated with corporate severance packages.\n\n\n\nQuery Used for Retrieving Relevant Posts in Brandwatch\n\n\nWe compiled two datasets: one from April 8‚Äì10, 2022, capturing impressions from Season 1 finale, and another from March 20‚Äì22, 2025, following the Season 2 finale. In total, the dataset contains 1,786 posts for 2022 and 4,091 for 2025.\nTo ensure compliance with GDPR regulations, and to avoid the inadvertent release of sensitive information, we consolidated the raw data to include only two columns: a unique post ID and the content of the message. The final dataset is publicly available\nWhether you‚Äôre a longtime fan of the show or encountering it for the first time, this example offers a valuable case study in analyzing sentiment in unstructured public opinion data. Because data was pulled from social media it also serves as a guideline for examining more challenging text corpora, due to informal language, use of slang, emojis, hashtags, and abbreviations, as well as their variable length, inconsistent formatting, and inclusion of multimedia elements like images, videos, and links. Yeah, all the challenges you will face in the ‚Äúreal world‚Äù of sentiment analysis of user-generated content.\nThis lesson is supplemented by a folder, containing: 1) a data file containing the posts we will analyze comments.csv, and 2) the worksheet.qmd where we will be coding, computing, and visualizing the output of our tasks.\n\n\n\n\n\n\n‚§µÔ∏è Download & Save Project File\n\n\n\nAccess this folder , ensure to download it, and save in an easy to locate place (e.g., Desktop).\n&lt;FIXME: replace it with zenodo doi&gt;",
    "crumbs": [
      "Setup & Dataset"
    ]
  },
  {
    "objectID": "07_stopwords.html",
    "href": "07_stopwords.html",
    "title": "Removing Stop words",
    "section": "",
    "text": "Stop words are commonly occurring words that are usually filtered out during natural language processing, as they carry minimal semantic weight and are not as useful for feature extraction.\nExamples include articles (i.e., a, an, the), prepositions (e.g., in, on, at), conjunctions (and, but, or), and pronouns (they, she, he), but the list goes on. While they appear often in text, they usually don‚Äôt add significant meaning to a sentence or search query.\nBy ignoring stop words, search engines, databases, chatbots and virtual assistants can improve the speed of crawling and indexing and help deliver faster, more efficient results. Similar posistive effects applies to other NLP tasks and models performance, including sentiment analysis.\nFor this workshop, we will be using the package stopwords (more info) which is considered a ‚Äúon-stop stopping‚Äù for R users. For English language, the package relies on the Snowball list. But, before we turn to our worksheet to see how that process looks like and how it will apply to our data, let‚Äôs have a little challenge!\n\n\n\n\n\n\nüß† Knowledge Check\n\n\n\nHow many stop words can you spot in each of the following sentences:\n\nThe cat was sitting on the mat near the window.\nShe is going to the store because she needs some milk.\nI will be there in the morning if it doesn‚Äôt rain.\nThey have been working on the project for several days.\nAlthough he was tired, he continued to walk until he reached the house.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1. The cat was sitting on the mat near the window.\n2. She is going to the store because she needs some milk.\n3. I will be there in the morning, if it does not rain.\n4. They have been working on the project for several days.\n5. Although he was very tired, he continued to walk until he reached the house.\n\n\n\n\n\nYou might be wondering: Wait‚Ä¶ why are words like ‚Äúvery‚Äù and ‚Äúnot‚Äù excluded? Aren‚Äôt they important for sentiment analysis? The answer is: YES, they are! While these words are included in many stop word lists provided by standard NLP packages, we can customize the list to retain specific words we consider crucial for our analysis.\nNow, let‚Äôs return to the worksheet and see how we can put that into practice.\n\n\n\n\n\n\nüìë Suggested Reading\n\n\n\nCheck out this blog post for a summary of the history of stop words, discussion on its applications and some perspectives on developments in the age of AI.\nGaviraj, K. (2025, April 24). The origins of stop words. BytePlus. https://www.byteplus.com/en/topic/400391?title=the-origins-of-stop-words",
    "crumbs": [
      "Text Preprocessing",
      "Removing Stop Words"
    ]
  },
  {
    "objectID": "12_considerations.html",
    "href": "12_considerations.html",
    "title": "Some Considerations & Caveats",
    "section": "",
    "text": "Sentiment analysis, while a powerful method to extract insights from data, is far from perfect or straightforward. After all, it seeks to interpret natural language, which is constantly evolving. Speaking of evolution, did you know that the Cambridge dictionary added 6,000 words only this year, including ‚Äúbroligarchy‚Äù and ‚Äúdelulu‚Äù, many of which are widely used by Gen Alpha? This constant expansion of language highlights just how dynamic the texts we analyze can be.\nLanguage is also inherently rich, ambiguous, and culturally nuanced. Lexicon-based approaches, for instance, rely on predefined word lists and often struggle to capture subtleties in human expression.\nIn practice, sentiment analysis encounters issues like code-switching, where people mix languages in a single post, or compound sentences with mixed sentiments, such as ‚ÄúThe movie had great acting, but the ending was lame,‚Äù which are difficult to score accurately. Context dependence further complicates interpretation: words can flip polarity depending on the domain, like ‚Äúcheap,‚Äù which is positive when describing flights or monetary advantage in general, but negative when describing fabric or referring to quality.\nTemporal dynamics also play a role, as slang and cultural references evolve rapidly, e.g., ‚Äúbad‚Äù meaning ‚Äúgood‚Äù in some communities. Ambiguity adds another layer of difficulty: polysemous words like ‚Äúsick‚Äù can mean either ‚Äúill‚Äù or ‚Äúawesome‚Äù.\nAnother complication is to deal with sarcasm and irony which can completely invert the intended sentiment as in: ‚ÄúOh great, another awesome Monday morning traffic jam!‚Äù.\nImplicit sentiment may be present even when emotional words are absent, as in ‚ÄúThe waiter ignored us for 30 minutes before taking our order.‚Äù These factors collectively make sentiment analysis a useful but inherently imperfect tool for understanding human language and emotion.\nHowever, it is important to emphasize that as described before, we have only explored sentiment analysis through a lexicon-based approach, and that, as illustrated in Figure ? below, there are other methods, including machine learning, deep learning and their combination (hybrid), that can be employed to extract emotions from text, including user generated content, all with their own limitations and challenges.\nFor example, Amazon relies on deep learning algorithms to determine the sentiment of customer reviews by identifying positive, negative, or neutral tones in the text. The models are trained on a vast dataset of Amazon‚Äôs product descriptions and reviews and are regularly updated with new information. This robust approach enables Amazon to efficiently analyze and interpret customer feedback on a large scale.\nWhile there are more advanced approaches to sentiment analysis, including AI-assisted methods, these are discussion topics for future workshops!",
    "crumbs": [
      "Sentiment Analysis",
      "Some Considerations & Caveats"
    ]
  },
  {
    "objectID": "05_normalization.html",
    "href": "05_normalization.html",
    "title": "Normalization & Noise Reduction",
    "section": "",
    "text": "Now that we‚Äôre familiar with the dataset and have briefly discussed the essential preprocessing steps, let‚Äôs take a deeper dive into the normalization process.\nAs we‚Äôve seen, the main goal of normalization is to remove irrelevant punctuation and content, and to standardize the data in order to reduce noise. Below are some key actions we‚Äôll be performing during this workshop:\nA caveat when working with emojis is that they are figurative and highly contextual. Also, there may be important generational and cultural variability in how people interpret them. For example, some countries may use the Folded Hands Emoji (üôè) as a sign of thank you where others may seem as religious expression. Also, some may use it in a more positive way as gratitude, hope or respect, or in a negative context, where they might be demonstrating submission or begging.\nYou might have noticed based on the example above that emojis are converted to their equivalent CLDR (common, human-readable name) based on this emoji unicode list, which are not as nuanced and always helpful to detect sentiment. While not always perfect, that is a required step to normalize the data.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "05_normalization.html#excercise",
    "href": "05_normalization.html#excercise",
    "title": "Normalization & Noise Reduction",
    "section": "üèãÔ∏èExcercise",
    "text": "üèãÔ∏èExcercise\nIn pairs or groups of three, identify the techniques you would consider using to normalize and reduce noise in the following sentence.\n‚ÄúOMG!! üò± I can‚Äôt believe it‚Ä¶ This is CRAZY!!! #unreal ü§Ø‚Äù\n\nHow many techniques could you identify?\n\nBingo if you got all five!\nHere is an example how the final text would look like:\nomg [face scream in fear] I can not believe it this is crazy unreal [exploding head]",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "10_polarity.html",
    "href": "10_polarity.html",
    "title": "Polarity Analysis",
    "section": "",
    "text": "Now that we have completed all the key preprocessing steps and our example dataset is in much better shape, we can finally proceed with sentiment analysis.\nThis will be a two-step approach, where we will start with the basic polarity and then, advance to a more fine-grained analysis, exploring different emotional expressions within text.",
    "crumbs": [
      "Sentiment Analysis",
      "Polarity Analysis"
    ]
  },
  {
    "objectID": "10_polarity.html#basic-polarity",
    "href": "10_polarity.html#basic-polarity",
    "title": "Basic Polarity",
    "section": "Basic Polarity",
    "text": "Basic Polarity\nPolarity classification is a fundamental aspect of sentiment analysis to measure the overall emotional tone expressed in text data which can be categorized as positive, neutral or negative.\nMost models assign ‚Äúsentiment scores‚Äù with values ranging from -1 to +1 to represent the intensity of the sentiment, being scores closer to -1 considered negative, those closer to 0 neutral and +1 positive.\n\nWe will be using the package sentimentr (more info) to compute polarity classification and attribute sentiment scores to the posts included in our dataset. We chose this package due to its capability of handling negation (when negators appear nearby) and adjust to adverbs‚Äô magnitude also known as valence shifters, which are adverbs and words that modify the intensity or direction of sentiment. These include amplifiers (e.g., very, really, absolutely, totally), de-amplifiers or downtoners (e.g., barely, hardly, rarely, ), as well as adversative conjunctions (e.g., but, yet, however, although).\nFor example, this package is capable of",
    "crumbs": [
      "Detecting Reactions & Emotions",
      "Basic Polarity"
    ]
  },
  {
    "objectID": "11_emotion-detection.html",
    "href": "11_emotion-detection.html",
    "title": "Emotion Detection",
    "section": "",
    "text": "Emotion detection is another NLP technique aimed at identifying and quantifying human emotions expressed in text, which builds directly on traditional sentiment polarity analysis focusing on capturing more nuanced emotional states. While polarity classification identifies whether a text expresses positive, negative, or neutral sentiment, it does not capture the specific type of emotion behind that sentiment. For example, two negative texts could express very different emotions‚Äîone might convey anger, while another reflects sadness. By extending polarity into multiple emotional dimensions, emotion detection provides more granular and more actionable insights into how people truly feel.\nWe will use the syuzhet package (more info) to to help us classify emotions detected in our dataset. The name ‚Äúsyuzhet‚Äù is inspired by the work of Russian Formalists Victor Shklovsky and Vladimir Propp, who distinguished between two aspects of a narrative: the fabula and the syuzhet. The fabula represents the chronological sequence of events, while the syuzhet refers to the way these events are presented or structured; the narrative‚Äôs technique or ‚Äúdevice.‚Äù In other words, syuzhet focuses on how the story (fabula) is organized and conveyed to the audience.\nThe syuzhet package implements the National Research Council Canada (NRC) Emotion Lexicon which maps words to basic emotions, in addition to polarity scores, allowing for fine-grained emotion scoring at the word, sentence, or document level.\nThis framework uses eight categories of emotions based on Robert Plutchik‚Äôs theory of the emotional wheel, a foundational model that illustrates the relationships between human emotions from a psychological perspective. Plutchik‚Äôs wheel identifies eight primary emotions: anger, disgust, sadness, surprise, fear, trust, joy, and anticipation. As illustrated in Figure ? below, these emotions are organized into four pairs of opposites on the wheel. Emotions positioned diagonally across from each other represent opposites, while adjacent emotions share similarities, reflecting a positive correlation.\n\n\n\nFigure?. Plutchik‚Äôs wheel of emotions. Image from: Zeng, X., Chen, Q., Chen, S., & Zuo, J. (2021). Emotion label enhancement via emotion wheel and lexicon. Mathematical Problems in Engineering, 2021(1), 6695913. https://doi.org/10.1155/2021/6695913\n\n\nThe NRC Emotion Lexicon was developed as part of research into affective computing and sentiment analysis using a combination of manual annotation and crowdsourcing. Human annotators evaluated thousands of words, indicating which emotions were commonly associated with each word. This method ensured that the lexicon captured human-perceived emotional associations, rather than relying solely on statistical co-occurrences in text.\nSince its release, the NRC Emotion Lexicon has become a widely used resource in computational social science, marketing analytics, and text mining, because it allows researchers to move beyond simple positive/negative polarity to fine-grained emotion detection, making it possible to analyze the emotional content of text at scale.\nYou may explore NRC‚Äôs lexicon Tableau dashboard to explore words associated with each emotion category:\n\nYou might be wondering: if the syuzhet package also computes polarity, why did we choose sentimentr in our pipeline? The reason is that syuzhet does not inherently account for valence shifters. In the original syuzhet implementation, words are scored in isolation‚Äîso ‚Äúgood‚Äù = +1, ‚Äúbad‚Äù = ‚àí1‚Äîregardless of nearby negations or intensifiers. For example, ‚Äúnot good‚Äù would still be counted as +1. Because sentimentr adjusts sentiment scores for negators and amplifiers, polarity results are more nuanced, robust, and reliable.",
    "crumbs": [
      "Sentiment Analysis",
      "Emotion Detection"
    ]
  },
  {
    "objectID": "12_considerations.html#references",
    "href": "12_considerations.html#references",
    "title": "Some Considerations & Caveats",
    "section": "References",
    "text": "References\nMao, Y., Liu, Q., & Zhang, Y. (2024). Sentiment analysis methods, applications, and challenges: A systematic literature review. Journal of King Saud University - Computer and Information Sciences, 36(4), 102048. https://doi.org/10.1016/j.jksuci.2024.102048",
    "crumbs": [
      "Sentiment Analysis",
      "Some Considerations & Caveats"
    ]
  },
  {
    "objectID": "05_normalization.html#expanding-contractions",
    "href": "05_normalization.html#expanding-contractions",
    "title": "Normalization & Noise Reduction",
    "section": "Expanding Contractions",
    "text": "Expanding Contractions\nAnother important step is to properly handle contractions. In everyday language, we often shorten words: can‚Äôt, don‚Äôt, it‚Äôs. These make speech and writing flow more easily, but they can cause confusion for Natural Language Processing (NLP) models. Expanding contractions, such as changing can‚Äôt to cannot or it‚Äôs to it is, helps bring clarity and consistency to the text because NLP models treat don‚Äôt and do not as completely different words, even though they mean the same thing. Also, words like cant, doesnt, and whats lose their meaning. Expanding contractions reduces this inconsistency and ensures that both forms are recognized as the same concept. Expanding it to is not happy makes the negative sentiment explicit, which is especially important in tasks like sentiment analysis.\nSo, while it may seem like a small step, it often leads to cleaner data, leaner models, and more accurate results. First, however, we need to ensure that apostrophes are handled correctly. It‚Äôs not uncommon to encounter messy text where nonstandard characters are used in place of the straight apostrophe (‚Äô). Such inconsistencies are very common and can disrupt contraction expansion.\n\n\n\n\n\n\n\n\nCharacter\nUnicode\nNotes\n\n\n\n\n'\nU+0027\nStandard straight apostrophe, used in most dictionaries\n\n\n‚Äô\nU+2019\nRight single quotation mark (curly apostrophe)\n\n\n‚Äò\nU+2018\nLeft single quotation mark\n\n\n º\nU+02BC\nModifier letter apostrophe\n\n\n`\nU+0060\nGrave accent (sometimes typed by mistake)\n\n\n\nAlright, let‚Äôs go back to our worksheet to get our hands ‚Äúdirty‚Äù with some cleaning and normalization, helping us make it more normalized, consistent, and ready for analysis.\n# Normalization\n\n# Normalize apostrophes\ncomments$text &lt;- gsub(\"[‚Äô‚Äò º`]\", \"'\", comments$text)\n\n# Expand contractions\ncomments &lt;- comments %&gt;%\n  mutate(text_expand = replace_contraction(text))\n\n# Convert to lowercase\ncomments &lt;- comments %&gt;%\n  mutate(text_lower = tolower(text_expand))\n\n# Remove URLs\ncomments &lt;- comments %&gt;%\n  mutate(text_nourl = str_replace_all(text_lower, \"http[s]?://[^\\\\s,]+|www\\\\.[^\\\\s,]+\", \"\"))\n\n# Remove mentions (@username)\ncomments &lt;- comments %&gt;%\n  mutate(text_nomention = str_replace_all(text_nourl, \"@[A-Za-z0-9_]+\", \"\"),\n         text_nomention = str_squish(text_nomention))\n\n# Remove punctuation & numbers\ncomments &lt;- comments %&gt;%\n  mutate(text_cleaned = text_nomention %&gt;%\n           str_replace_all(\"[[:punct:]‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶|]\", \"\") %&gt;%\n           #str_replace_all(\"[^\\\\w\\\\s.!?]\", \"\") %&gt;% #preserves sentence boundaries \n           str_replace_all(\"[[:digit:]]\", \"\") %&gt;%\n           str_replace_all(\"(.)\\\\1{2,}\", \"\\\\1\") %&gt;%  # collapse rep. letters eg. \"Amaaazing\"\n           str_squish()\n  )\n\n# Convert emojis to text\nemoji_dict &lt;- emo::jis[, c(\"emoji\", \"name\")]\nemoji_dict$name &lt;- paste0(\"[\", emoji_dict$name, \"]\")\n\nreplace_emojis &lt;- function(text, emoji_dict) {\n  stri_replace_all_fixed(\n    str = text, \n    pattern = emoji_dict$emoji, \n    replacement = emoji_dict$name, \n    vectorize_all = FALSE\n  )\n}\n\ncomments &lt;- comments %&gt;%\n  mutate(text_noemoji = replace_emojis(text_cleaned, emoji_dict),\n         text = str_replace_all(text_noemoji, \"[^[:alpha:][:space:]]\", \"\"),\n         text = str_squish(text))\n\n# Save normalized data\nwrite_csv(comments %&gt;% select(id, text), \"normalized.csv\")\n\n\n\n\n\n\nüìë Suggested Readings\n\n\n\nBai, Q., Dan, Q., Mu, Z., & Yang, M. (2019). A systematic review of emoji: Current research and future perspectives. Frontiers in psychology, 10, https://doi.org/10.3389/fpsyg.2019.02221\nGraham, P. V. (2024). Emojis: An Approach to Interpretation. UC L. SF Commc‚Äôn and Ent. J., 46, 123. https://repository.uclawsf.edu/cgi/viewcontent.cgi?article=1850&context=hastings_comm_ent_law_journal",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "10_polarity.html#polarity-classification",
    "href": "10_polarity.html#polarity-classification",
    "title": "Polarity Analysis",
    "section": "Polarity Classification",
    "text": "Polarity Classification\nPolarity classification is a fundamental aspect of sentiment analysis to measure the overall emotional tone expressed in text data which can be categorized as positive, neutral or negative.\nMost models assign ‚Äúsentiment scores‚Äù with values ranging from -1 to +1 to represent the intensity of the sentiment, being scores closer to -1 considered negative, those closer to 0 neutral and +1 positive.\n\nWe will be using the package sentimentr (more info) to compute polarity classification and attribute sentiment scores to the posts included in our dataset.\nTraditional sentiment analysis techniques assign polarity by matching words against dictionaries labeled as ‚Äúpositive,‚Äù ‚Äúnegative,‚Äù or ‚Äúneutral.‚Äù While straightforward, this approach is overly simplistic: it ignores context and flattens the richness of our syntactically complex, lexically nuanced language, that transcends individual words. The sentimentr package extends lexicon-based methods by accounting for valence shifters; words that subtly alter sentiment.\nThe package includes 130 valence shifters that can reverse or modulate the sentiment indicated by standard dictionaries. These fall into four main categories: negators (e.g., not, can‚Äôt), amplifiers (e.g., very, really, absolutely, totally, certainly), de-amplifiers or down-toners (e.g., barely, hardly, rarely, almost), and adversative conjunctions (e.g., although, however, but, yet, that being said). This refinement is important because simple dictionary lookups miss the nuanced meaning.\nIn summary, each word in a sentence is checked against a dictionary of positive and negative words, like the Jockers dictionary in the lexicon package. Words that are positive get a +1, and words that are negative get a -1, which are called polarized words. Around each polarized word, we look at the nearby words (usually four before and two after) to see if they change the strength or direction of the sentiment. This group of words is called a polarized context cluster. Words in the cluster can be neutral, negators (like ‚Äúnot‚Äù), amplifiers (like ‚Äúvery‚Äù), or de-amplifiers (like ‚Äúslightly‚Äù). Neutral words don‚Äôt affect the sentiment but still count toward the total word count.\nThe main polarized word‚Äôs sentiment is then adjusted by the surrounding words. Amplifiers make it stronger, de-amplifiers make it weaker, and negators can flip the sentiment. Multiple negators can cancel each other out, like ‚Äúnot unhappy‚Äù turning positive.\nWords like ‚Äúbut,‚Äù ‚Äúhowever,‚Äù and ‚Äúalthough‚Äù also influence the sentiment. Words before these are weighted less, and words after them are weighted more because they signal a shift in meaning. Finally, all the adjusted scores are combined and scaled by the sentence length to give a final sentiment score for the sentence.\nWith this approach, we can explore more confidently whether the show‚Äôs viewers felt positive, neutral, or negative about it.",
    "crumbs": [
      "Sentiment Analysis",
      "Polarity Analysis"
    ]
  },
  {
    "objectID": "05_normalization.html#exercise",
    "href": "05_normalization.html#exercise",
    "title": "Normalization & Noise Reduction",
    "section": "üèãÔ∏è Exercise",
    "text": "üèãÔ∏è Exercise\nIn pairs or groups of three, identify the techniques you would consider using to normalize and reduce noise in the following sentence:\n‚ÄúOMG!! üò± I can‚Äôt believe it‚Ä¶ This is CRAZY!!! #unreal ü§Ø‚Äù\n\n\n\n\n\n\nNote\n\n\n\n\n\nSolution (click to reveal):\nHow many techniques could you identify? Bingo if you have spotted four! After applying them the sentence should look like:\nomg [face scream in fear] I can not believe it this is crazy unreal [exploding head]\n\n\n\nA caveat when working with emojis is that they are figurative and highly contextual. Also, there may be important generational and cultural variability in how people interpret them. For example, some countries may use the Folded Hands Emoji (üôè) as a sign of thank you where others may seem as religious expression. Also, some may use it in a more positive way as gratitude, hope or respect, or in a negative context, where they might be demonstrating submission or begging.\nYou might have noticed based on the example above that emojis are converted to their equivalent CLDR (common, human-readable name) based on this emoji unicode list, which are not as nuanced and always helpful to detect sentiment. While not always perfect, that is a required step to normalize the data.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  }
]