---
title: "Normalization & Noise Reduction"
editor: 
  markdown: 
    wrap: 72
---

Now that we‚Äôre familiar with the dataset and have briefly discussed the
essential preprocessing steps, let‚Äôs take a deeper dive into the
normalization process.

As we've seen, the main goal of normalization is to remove irrelevant
punctuation and content, and to standardize the data in order to reduce
noise. Below are some key actions we‚Äôll be performing during this
workshop:

| Action                       | Why it matters?                                                                                                                                                                                                                                                                                                                                                         |
|-------------|-----------------------------------------------------------|
| Remove URLs                  | URLs often contain irrelevant noise and don't contribute meaningful content for analysis.                                                                                                                                                                                                                                                                               |
| Remove Punctuation & Symbols | Punctuation marks and other symbols including those extensively used in social media for mentioning (\@) or tagging (#) rarely adds value in most NLP tasks and can interfere with tokenization (as we will cover in a bit) or word matching.                                                                                                                           |
| Remove Numbers               | Numbers can be noise in most contexts unless specifically relevant (e.g., in financial or medical texts) don't contribute much to the analysis. However, in NLP tasks they are considered important, there might be considerations to replace them with dummy tokens (e.g. \<NUMBER\>), or even converting them into their written form (e.g, 100 becomes one hundred). |
| Normalize Whitespaces        | Ensures consistent word boundaries and avoids issues during tokenization or frequency analysis.                                                                                                                                                                                                                                                                         |
| Convert to Lowercase         | Prevents case sensitivity from splitting word counts due to case variations (e.g., ‚ÄúAppleTV‚Äù ‚â† "APPLETV" ‚â† ‚ÄúappleTV‚Äù ‚â† ‚Äúappletv‚Äù), improving model consistency.                                                                                                                                                                                                         |
| Convert Emojis to Text       | Emojis play a unique role in text analysis, as they often convey sentiment. Rather than removing them, we will convert them into their corresponding text descriptions.                                                                                                                                                                                                 |

::: {.callout-note icon="false"}
## üß† Knowledge Check

In pairs or groups of three, identify the techniques you would consider
using to normalize and reduce noise in the following sentence:

*"OMG!! üò± I can't believe it... This is CRAZY!!! #unreal ü§Ø"*

::: {.callout-note icon="false" collapse="true"}
## Solution

How many techniques could you identify? Bingo if you have spotted four!

After applying them the sentence should look like:

*omg \[face scream in fear\] I can not believe it this is crazy unreal
\[exploding head\]*
:::
:::

A caveat when working with emojis is that they are figurative and highly
contextual. Also, there may be important generational and cultural
variability in how people interpret them. For example, some countries
may use the Folded Hands Emoji (üôè) as a sign of thank you where others
may seem as religious expression. Also, some may use it in a more
positive way as gratitude, hope or respect, or in a negative context,
where they might be demonstrating submission or begging.

You might have noticed based on the example above that emojis are
converted to their equivalent CLDR (common, human-readable name) based
on this [emoji unicode
list](https://www.unicode.org/emoji/charts/full-emoji-list.html), which
are not as nuanced and always helpful to detect sentiment. While not
always perfect, that is a required step to normalize the data.

## Dealing with Contractions

Another important step is to properly handle contractions. In everyday
language, we often shorten words: *can‚Äôt*, *don‚Äôt*, *it‚Äôs*. These make
speech and writing flow more easily, but they can cause confusion for
Natural Language Processing (NLP) models. Expanding contractions, such
as changing *can‚Äôt* to *cannot* or *it‚Äôs* to *it is*, helps bring
clarity and consistency to the text because NLP models treat *don‚Äôt* and
*do not* as completely different words, even though they mean the same
thing. Also, words like *cant*, *doesnt*, and *whats* lose their
meaning. Expanding contractions reduces this inconsistency and ensures
that both forms are recognized as the same concept. Expanding it to *is
not happy* makes the negative sentiment explicit, which is especially
important in tasks like sentiment analysis.

So, while it may seem like a small step, it often leads to cleaner data,
leaner models, and more accurate results. First, however, we need to
ensure that apostrophes are handled correctly. It's not uncommon to
encounter messy text where nonstandard characters are used in place of
the straight apostrophe ('). Such inconsistencies are very common and
can disrupt contraction expansion.

| Character | Unicode | Notes                                                   |
|-------------|-------------|----------------------------------------------|
| `'`       | U+0027  | Standard straight apostrophe, used in most dictionaries |
| `‚Äô`       | U+2019  | Right single quotation mark (curly apostrophe)          |
| `‚Äò`       | U+2018  | Left single quotation mark                              |
| ` º`       | U+02BC  | Modifier letter apostrophe                              |
| `` ` ``   | U+0060  | Grave accent (sometimes typed by mistake)               |

Alright, let's go back to our worksheet to get our hands "dirty" with
some cleaning and normalization, helping us make it more normalized,
consistent, and ready for analysis.

::: callout-caution
## The order matters!

In text preprocessing, **the order of steps matters** because each
transformation changes the text in a way that can affect subsequent
steps. Doing things in a different order can lead to different results,
and sometimes even incorrect or unexpected outcomes. For example, if we
**remove punctuation before expanding contractions**, `"can't"` might
turn into `"cant"` instead of `"cannot"`, losing the correct meaning.
:::

## 1. Handling apostrophes

The gsub() function is part of base R and stands for global substitute
and is used to search for patterns in text and replace them with
something else. Its basic form is gsub(pattern, replacement, x), where
pattern is the regular expression you want to match, replacement is the
text you want to insert, and x is the character vector you‚Äôre applying
it to. In this case, the pattern "\[‚Äô‚Äò º\]" looks for several different
kinds of apostrophes and backticks‚Äîlike the left and right single
quotes, the modifier apostrophe, and the backtick. Each of those gets
replaced with a simple, standard apostrophe ('\`).

This step helps clean up text by making sure all apostrophes are
consistent, rather than a mix of fancy Unicode versions. Apllying it to
the `text` column in our `comments` dataset should look like.

``` r
comments$text <- gsub("[‚Äô‚Äò º`]", "'", comments$text)
```

## 2. Expanding contractions

Let's first make sure that words like "don't" become "do not". We will
also create a new column inside the comments dataset called
`text_expand`:

```r
comments <- comments %>%
  mutate(text_expand = replace_contraction(text))
```

## 3. Convert to lowercase

Having all text converted to lowercase will be our next step, using the
mutate function we will add the following code which will create a new
`text_lower` column:

```r
comments <- comments %>%
  mutate(text_lower = tolower(text_expand))
```

## 4. Remove URLs

We have a few URLs in our dataset and because they vary in format (e.g.,
http://, https://, or www.), we need to provide a regular expression
that can take care of these variations and save it as `text_nourl`.

```r
comments <- comments %>%
  mutate(text_nourl = str_replace_all(text_lower, "http[s]?://[^\\s,]+|www\\.[^\\s,]+", ""))
```

Breaking it down:

-   `http[s]?://` : Matches http:// or https://, where `[s]?`: means
    optional "s" (so it matches both http and https).

-   `[^\\s,]+`: where \[\^...\] is a negated character class, meaning
    "match anything not in here", `\\s` refers to whitespace (space,
    tab, newline), and `,` to match one or more characters that are not
    spaces or commas, that in combination with the above, matches the
    entire URL up to a space or comma.

-   `|` : Means OR in regex. So it matches either the first pattern or
    the second.

-   `www\\.[^\\s,]+` : Matches URLs that start with www. and continue
    until a space or comma.

## 5. Remove mentions & Handling Extra Space

Continuing with our workflow, we will now handle direct mentions and
usernames in our dataset, as they do not contribute relevant information
to our analysis. We will use a function to replace all occurrences of
usernames preceded by an \@ symbol.

```r
comments <- comments %>% 
  mutate(
    text_nomention = str_replace_all(text_nourl, "@[A-Za-z0-9_]+", ""), 
```

## 6. Remove punctuation, numbers, repeated letters and extra spaces

Alright, time to remove punctuation, numbers and repeated letters used
for emphasis, that might affect our analysis (e.g. "Amaaaazing",
"Loooove") and take care of some extra spaces.

```r
comments <- comments %>%
  mutate(
    text_cleaned = text_nomention %>%
      str_replace_all("[[:punct:]‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶|[:digit:]]", "") %>%  # remove punctuation & digits
      str_replace_all("(.)\\1{2,}", "\\1") %>%                # collapse repeated letters
      str_squish()                                             # trim & remove extra spaces
  )
```

The punctuation regular expression:

-   `[:punct:]`: matches all standard punctuation characters (e.g., ! "
    \# \$ % & ' ( ) \* + , - . / : ; \< = \> ? \@ \[ ¬†\] \^ \_ { \| }
    \~).

-   `‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶` : matches special Unicode punctuation, like curly quotes,
    en-dash, em-dash, ellipsis.

-   `|`: matches the literal pipe symbol.

-   `[:digit:]`: matches any digit (0‚Äì9).

-   For the collapsing part, `(.)`: captures any single character and
    stores it in a capture group.

-   `\\1`: refers back to the first captured character.

-   `{2,}`: matches 2 or more consecutive occurrences of that character.

Together, `(.)\\1{2,}` matches any character repeated three or more
times in a row and `str_replace_all` replaces the repeated sequence with
just one occurrence of the character.

We have created multiple intermediate columns to facilitate the
inspection of each step, but we could have streamlined the pipeline with
a more slim approach, like the one below:

```r
comments <- comments %>%
  mutate(
    text_cleaned = text %>%
      # Normalize apostrophes
      str_replace_all("[‚Äô‚Äò º`]", "'") %>%
      # Expand contractions
      replace_contraction() %>%
      # Convert to lowercase
      tolower() %>%
      # Remove URLs
      str_replace_all("http[s]?://[^\\s,]+|www\\.[^\\s,]+", "") %>%
      # Remove mentions
      str_replace_all("@[A-Za-z0-9_]+", "") %>%
      # Remove punctuation & numbers, collapse repeated letters, trim spaces
      str_replace_all("[[:punct:]‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶|]", "") %>%
      str_replace_all("[[:digit:]]", "") %>%
      str_replace_all("(.)\\1{2,}", "\\1") %>%
      str_squish()
  )
```

## 7. Convert emojis to text

Okay, now we‚Äôll convert emojis into their text descriptions to make them
machine-readable, using the emoji package to help with this step:

```r
# Load the emoji dictionary where 'jis' contains a table with 'emoji' and 'name' columns.
emoji_dict <- emo::jis[, c("emoji", "name")]

# Optional(e.g., üòÄ becomes [grinning face])
emoji_dict$name <- paste0("[", emoji_dict$name, "]")

# Define a function to replace emojis in text with their corresponding names.
replace_emojis <- function(text, emoji_dict) {
  stri_replace_all_fixed(
    str = text,                  # The text to process
    pattern = emoji_dict$emoji,  # The emojis to find
    replacement = emoji_dict$name, # Their corresponding names
    vectorize_all = FALSE        # element-wise replacement in a same string
  )
}

comments <- comments %>%
  mutate(text_noemoji = replace_emojis(text_cleaned, emoji_dict),
         # Remove any remaining non-alphabetic characters (like numbers, punctuation, etc.)
         text = str_replace_all(text_noemoji, "[^[:alpha:][:space:]]", ""),
         # Remove extra whitespace between words
         text = str_squish(text))

# Save the normalized text to a CSV file with only 'id' and 'text' columns
write_csv(comments %>% select(id, text), "normalized.csv")
```

Have you had a chance to look at the emoji dictionary we loaded into our
RStudio environment? It‚Äôs packed with more emojis and some surprising
meanings than you might expect.

With emojis taken care of, we can now move on to the next preprocessing
step: tokenization.

::: {.callout-note icon="false"}
## üìë Suggested Readings

Bai, Q., Dan, Q., Mu, Z., & Yang, M. (2019). A systematic review of
emoji: Current research and future perspectives. *Frontiers in
psychology*, *10*, <https://doi.org/10.3389/fpsyg.2019.02221>

Graham, P. V. (2024). Emojis: An Approach to Interpretation. *UC L. SF
Commc'n and Ent. J.*, *46*, 123.
<https://repository.uclawsf.edu/cgi/viewcontent.cgi?article=1850&context=hastings_comm_ent_law_journal>
:::