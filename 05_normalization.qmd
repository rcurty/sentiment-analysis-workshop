---
title: "Normalization & Noise Reduction"
---

Now that we‚Äôre familiar with the dataset and have briefly discussed the essential preprocessing steps, let‚Äôs take a deeper dive into the normalization process.

As we've seen, the main goal of normalization is to remove irrelevant punctuation and content, and to standardize the data in order to reduce noise. Below are some key actions we‚Äôll be performing during this workshop:

| Action                       | Why it matters?                                                                                                                                                                                                                                                                                                                                                         |
|------------------|------------------------------------------------------|
| Remove URLs                  | URLs often contain irrelevant noise and don't contribute meaningful content for analysis.                                                                                                                                                                                                                                                                               |
| Remove Punctuation & Symbols | Punctuation marks and other symbols including those extensively used in social media for mentioning (\@) or tagging (#) rarely adds value in most NLP tasks and can interfere with tokenization (as we will cover in a bit) or word matching.                                                                                                                           |
| Remove Numbers               | Numbers can be noise in most contexts unless specifically relevant (e.g., in financial or medical texts) don't contribute much to the analysis. However, in NLP tasks they are considered important, there might be considerations to replace them with dummy tokens (e.g. \<NUMBER\>), or even converting them into their written form (e.g, 100 becomes one hundred). |
| Normalize Whitespaces        | Ensures consistent word boundaries and avoids issues during tokenization or frequency analysis.                                                                                                                                                                                                                                                                         |
| Convert to Lowercase         | Prevents case sensitivity from splitting word counts due to case variations (e.g., ‚ÄúAppleTV‚Äù ‚â† "APPLETV" ‚â† ‚ÄúappleTV‚Äù ‚â† ‚Äúappletv‚Äù), improving model consistency.                                                                                                                                                                                                         |
| Convert Emojis to Text       | Emojis play a unique role in text analysis, as they often convey sentiment. Rather than removing them, we will convert them into their corresponding text descriptions.                                                                                                                                                                                                 |

::: {.callout-note icon="false"}
## üß† Knowledge Check

In pairs or groups of three, identify the techniques you would consider using to normalize and reduce noise in the following sentence:

*"OMG!! üò± I can't believe it... This is CRAZY!!! #unreal ü§Ø"*

::: {.callout-note icon="false" collapse="true"}
## Solution

How many techniques could you identify? Bingo if you have spotted four!

After applying them the sentence should look like:

*omg \[face scream in fear\] I can not believe it this is crazy unreal \[exploding head\]*
:::
:::

A caveat when working with emojis is that they are figurative and highly contextual. Also, there may be important generational and cultural variability in how people interpret them. For example, some countries may use the Folded Hands Emoji (üôè) as a sign of thank you where others may seem as religious expression. Also, some may use it in a more positive way as gratitude, hope or respect, or in a negative context, where they might be demonstrating submission or begging.

You might have noticed based on the example above that emojis are converted to their equivalent CLDR (common, human-readable name) based on this [emoji unicode list](https://www.unicode.org/emoji/charts/full-emoji-list.html), which are not as nuanced and always helpful to detect sentiment. While not always perfect, that is a required step to normalize the data.

## Expanding Contractions

Another important step is to properly handle contractions. In everyday language, we often shorten words: *can‚Äôt*, *don‚Äôt*, *it‚Äôs*. These make speech and writing flow more easily, but they can cause confusion for Natural Language Processing (NLP) models. Expanding contractions, such as changing *can‚Äôt* to *cannot* or *it‚Äôs* to *it is*, helps bring clarity and consistency to the text because NLP models treat *don‚Äôt* and *do not* as completely different words, even though they mean the same thing. Also, words like *cant*, *doesnt*, and *whats* lose their meaning. Expanding contractions reduces this inconsistency and ensures that both forms are recognized as the same concept. Expanding it to *is not happy* makes the negative sentiment explicit, which is especially important in tasks like sentiment analysis.

So, while it may seem like a small step, it often leads to cleaner data, leaner models, and more accurate results. First, however, we need to ensure that apostrophes are handled correctly. It's not uncommon to encounter messy text where nonstandard characters are used in place of the straight apostrophe ('). Such inconsistencies are very common and can disrupt contraction expansion.

| Character | Unicode | Notes                                                   |
|-----------------|-----------------|--------------------------------------|
| `'`       | U+0027  | Standard straight apostrophe, used in most dictionaries |
| `‚Äô`       | U+2019  | Right single quotation mark (curly apostrophe)          |
| `‚Äò`       | U+2018  | Left single quotation mark                              |
| ` º`       | U+02BC  | Modifier letter apostrophe                              |
| `` ` ``   | U+0060  | Grave accent (sometimes typed by mistake)               |

Alright, let's go back to our worksheet to get our hands "dirty" with some cleaning and normalization, helping us make it more normalized, consistent, and ready for analysis.

``` r
# Normalization

# Normalize apostrophes
comments$text <- gsub("[‚Äô‚Äò º`]", "'", comments$text)

# Expand contractions
comments <- comments %>%
  mutate(text_expand = replace_contraction(text))

# Convert to lowercase
comments <- comments %>%
  mutate(text_lower = tolower(text_expand))

# Remove URLs
comments <- comments %>%
  mutate(text_nourl = str_replace_all(text_lower, "http[s]?://[^\\s,]+|www\\.[^\\s,]+", ""))

# Remove mentions (@username)
comments <- comments %>%
  mutate(text_nomention = str_replace_all(text_nourl, "@[A-Za-z0-9_]+", ""),
         text_nomention = str_squish(text_nomention))

# Remove punctuation & numbers
comments <- comments %>%
  mutate(text_cleaned = text_nomention %>%
           str_replace_all("[[:punct:]‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶|]", "") %>%
           #str_replace_all("[^\\w\\s.!?]", "") %>% #preserves sentence boundaries 
           str_replace_all("[[:digit:]]", "") %>%
           str_replace_all("(.)\\1{2,}", "\\1") %>%  # collapse rep. letters eg. "Amaaazing"
           str_squish()
  )

# Convert emojis to text
emoji_dict <- emo::jis[, c("emoji", "name")]
emoji_dict$name <- paste0("[", emoji_dict$name, "]")

replace_emojis <- function(text, emoji_dict) {
  stri_replace_all_fixed(
    str = text, 
    pattern = emoji_dict$emoji, 
    replacement = emoji_dict$name, 
    vectorize_all = FALSE
  )
}

comments <- comments %>%
  mutate(text_noemoji = replace_emojis(text_cleaned, emoji_dict),
         text = str_replace_all(text_noemoji, "[^[:alpha:][:space:]]", ""),
         text = str_squish(text))

# Save normalized data
write_csv(comments %>% select(id, text), "normalized.csv")
```

::: {.callout-note icon="false"}
## üìë Suggested Readings

Bai, Q., Dan, Q., Mu, Z., & Yang, M. (2019). A systematic review of emoji: Current research and future perspectives. *Frontiers in psychology*, *10*, <https://doi.org/10.3389/fpsyg.2019.02221>

Graham, P. V. (2024). Emojis: An Approach to Interpretation. *UC L. SF Commc'n and Ent. J.*, *46*, 123. <https://repository.uclawsf.edu/cgi/viewcontent.cgi?article=1850&context=hastings_comm_ent_law_journal>
:::
